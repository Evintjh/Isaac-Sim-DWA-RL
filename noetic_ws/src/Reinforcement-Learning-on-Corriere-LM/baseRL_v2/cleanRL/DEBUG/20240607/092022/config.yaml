# Env
env_id: 'Jackal-v0'
jackal_asset_path : '/isaac-sim/Reinforcement-Learning-on-Corriere-LM/baseRL_v2/cleanRL/multiagent/jackal_multiagent.usda'
# jackal_asset_path : '/isaac-sim/Reinforcement-Learning-on-Corriere-LM/baseRL_v2/cleanRL/multiagent/jackal_1.usda'

# General
evaluation : 0
seed: 1                                   #, type=int, default=1,                    help=seed of the experiment)
physics_dt : 60.0                    #1.0/x ( if physics_dt == 10.0 -> 1/10.0)
rendering_dt : 30.0                  #1.0/x 
eval_physics_dt : 60.0 
skip_frame : 6                
grid_spacing : 10.5
num_envs: 1                          # Number of duplicate envs
num_agents: 64
write_interval: 10000                        # Write interval for tensorboard
eval_episodes: 5                         # Number of eposides to evaluate


model_save_interval: 20 # model save per x epoch
eval_interval : 30 # eval during training per x epoch
algo: PPO                                     # PPO or SAC
total_timesteps: 1e8                           #, type=int, default=1000000,                    help=total timesteps of the experiments)
collision_distance: 0.45 #493
goal_radius: 0.45
final_dist_reward: False
timeout: 1024
social_penalty: 0.0 # 0 to off. Default 0.05
gamma: 0.99                                   #, type=float, default=0.99,                    help=the discount factor gamma)
num_stacks: 3
w_max: 1
v_max: 1

# PPO 
ppo_print_interval: 1                        # print_interval
ppo_lr: 3e-4                                  # type=float, default=3e-4,    help="the learning rate of the optimizer")
num_steps: 1024               # for MA, num_steps >= 256                   # type=int, default=2048,    help="the number of steps to run in each environment per policy rollout")
anneal_lr: True                                  # type=lambda x: bool(strtobool(x)), default=True, nargs="? # const=True,    help="Toggle learning rate annealing for policy and value networks")
gae_lambda: 0.95                                  # type=float, default=0.95,    help="the lambda for the general advantage estimation")
num_minibatches: 32           # for MA, mini_bs <= 256                       # type=int, default=32,    help="the number of mini_batches")
update_epochs: 5                                  # type=int, default=10,    help="the K epochs to update the policy")
norm_adv: True                                  # type=lambda x: bool(strtobool(x)), default=True, nargs="? # const=True,    help="Toggles advantages normalization")
clip_coef: 0.1                                  # type=float, default=0.2,    help="the surrogate clipping coefficient")
clip_vloss: True                                  # type=lambda x: bool(strtobool(x)), default=True, nargs="? # const=True,    help="Toggles whether or not to use a clipped loss for the value function, as per the paper.")
ent_coef: 0.01                                  # type=float, default=0.0,    help="coefficient of the entropy")
vf_coef: 0.5                                  # type=float, default=0.5,    help="coefficient of the value function")
max_grad_norm: 0.5                                  # type=float, default=0.5,    help="the maximum norm for the gradient clipping")
target_kl: null                                  # type=float, default=None,    help="the target KL divergence threshold")


# minibs = num_steps*num_agents / num_minib <= 256

domain_randomization:
  randomize: False
  randomization_params:
    # simulation:
    #   gravity:
    #     on_reset:
    #       operation: "additive"
    #       distribution: "gaussian"
    #       distribution_parameters: [[0.0, 0.0, 0.0], [0.0, 0.0, 0.4]]
    articulation_views:
      jackal_view:
        # material_properties:
        #   on_reset:
        #     num_buckets: 250
        #     operation: "scaling"
        #     distribution: "uniform"
        #     distribution_parameters: [[0.7, 1, 1], [1.3, 1, 1]]  #
        damping :
          on_reset:
            operation: "additive"
            distribution: "uniform"
            distribution_parameters: [1,10]
        body_masses:
          on_reset:
            operation: "scaling"
            distribution: "uniform"
            distribution_parameters: [3,3]